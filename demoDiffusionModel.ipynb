{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffusion model\n",
    "Wenhao Zhang \\\n",
    "Feb 20, 2025\n",
    "\n",
    "#### Forward process\n",
    "$x_t = x_{t-1} + f dt + g dw$\n",
    "\n",
    "corresponding to\n",
    "\n",
    "$p(x_t|x_{t-1}) = \\mathcal{N}(x_t | x_{t-1} + f dt, g^2)$\n",
    "\n",
    "#### Backward process (also run along the time forward direction)\n",
    "\n",
    "$x_t = x_{t-1} + [f - g^2 \\nabla \\ln p(x_t)] dt + g dw$\n",
    "\n",
    "\n",
    "#### Forward process: posterior Langevin sampling\n",
    "In the neural ciruits, the continuous attractor network (CAN) runs the forward process to sample stimulus posteriors via running Langevin sampling dynamics.\n",
    "\n",
    "$x_t = x_{t-1} + \\tau^{-1} \\nabla \\ln \\pi(x_{t-1}) dt + \\sqrt{2\\tau^{-1}} dw$\n",
    "\n",
    "where $\\tau$ is the time constant of the Langevin sampling, and the posterior could be \n",
    "\n",
    "$\\pi(x) = \\mathcal{N}(x| \\mu, \\sigma^2)$,\n",
    "\n",
    "which leads to \n",
    "\n",
    "$\\nabla \\ln \\pi(x) = \\sigma^{-2} (\\mu - x)$\n",
    "\n",
    "And then the detailed Langevin sampling dynamics is \n",
    "\n",
    "$x_t = x_{t-1} + \\tau^{-1} \\sigma^{-2} (\\mu - x) dt + \\sqrt{2\\tau^{-1}} dw$\n",
    "\n",
    "which is equivalent to define\n",
    "\n",
    "$f = \\tau^{-1} \\sigma^{-2} (\\mu - x) dt$\n",
    "\n",
    "and \n",
    "\n",
    "$g = \\sqrt{2\\tau^{-1}}$\n",
    "\n",
    "in the forward process.\n",
    "\n",
    "\n",
    "#### Approximation of the score function in the circuit\n",
    "We propose the inputs from congruent neurons (modeled by a CAN) to opposite neurons approximate the score function via sampling. Specifically,\n",
    "\n",
    "$ p_{t}(x_t) = \\int p(x_t|x_{t-1}) p_{t-1}(x_{t-1}) dx_{t-1}$\n",
    "\n",
    "where \n",
    "\n",
    "$p(x_t|x_{t-1})$ is the transition probability defined by the forward process (Langevin sampling dynamics) in the model.\n",
    "Using sampling to approximate the above integration,\n",
    "\n",
    "$ p_t(x_t) \\approx \\frac{1}{N} \\sum_{i=1}^N p(x_t|\\tilde{x}_{t-1, i})$ where  $ \\tilde{x}_{t-1, i} \\sim p_{t-1}(x_{t-1})$\n",
    "\n",
    "We assume at each time, the neural circuits __only draw one sample\n",
    "__, and then the above equation reduces into\n",
    "\n",
    "$ p_t(x_t) \\approx p(x_t|\\tilde{x}_{t-1}) $\n",
    "\n",
    "#### Backward process with approximated socre function\n",
    "By using the above approximated score function, we can find\n",
    "\n",
    "$\n",
    "\\nabla _{x_t} \\ln p_t(x_t) \n",
    "\\approx \n",
    "\\nabla _{x_t} \\ln p(x_t|\\tilde{x}_{t-1})\n",
    "= g^{-2} (\\tilde{x}_{t-1} + fdt - x_t)\n",
    "$\n",
    "\n",
    "Then the backward process is \n",
    "\n",
    "$\n",
    "x_t \n",
    "\\approx x_{t-1} + [f - g^2 \\nabla \\ln p(x_t|x_{t-1})] dt + g dw \\\\\n",
    "= x_{t-1} + [f - (x_{t-1} + fdt - x_t)] dt + g dw \\\\\n",
    "= x_{t-1} (1-dt) + x_t dt + f(1-dt)dt + g dw \\\\\n",
    "\\approx x_{t-1} (1-dt) + (x_t + f ) dt + g dw\n",
    "$\n",
    "\n",
    "Note that the $x_t$ on the RHS is generated by the forward process (congruent neurons), which then applied to the backward process (opposite neurons) to generate the $x_t$ on the LHS.\n",
    "\n",
    "### Loss function (evidence lower bound)\n",
    "$E[\\log p(x_0)] \\geq E_q[\\log p(x_T) + \\sum_{t=1}^T \\log \\frac{p(x_{t-1}|x_t)}{q(x_t|x_{t-1})}]$\n",
    "\n",
    "$p(x_{t-1}|x_t) = \\frac{p(x_t|x_{t-1})p(x_{t-1})}{p(x_t)}$\n",
    "\n",
    "Therefore \n",
    "\n",
    "$\\sum_{t=1}^T \\log \\frac{p(x_{t-1}|x_t)}{q(x_t|x_{t-1})} = \n",
    "\\sum_{t=1}^T \\log \\frac{p(x_t|x_{t-1})}{q(x_t|x_{t-1})} \\frac{p(x_{t-1})}{p(x_t)}\n",
    "$\n",
    "\n",
    "Substituting into the original form,\n",
    "\n",
    "$E[\\log p(x_0)] \\geq E_q[\\log p(x_0) + \\sum_{t=1}^T \\log \\frac{p(x_t|x_{t-1})}{q(x_t|x_{t-1})}]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denoising score matching (DSM)\n",
    "\n",
    "The loss function of DSM is \n",
    "\n",
    "$J_{DSM}(\\mathbf{w}) = \\mathbb{E} _{q_\\sigma (\\tilde{x}, x)} ||s_\\mathbf{w}(\\tilde{x}) - \\nabla_{\\tilde{x}} \\ln q_\\sigma(\\tilde{x}|x)||^2$\n",
    "\n",
    "where $s_\\mathbf{w}(\\tilde{x})$ is the score network with parameter $\\mathbf{w}$, and $q_\\sigma(\\tilde{x}|x)$ is the transition probability for the noise perturbation.\n",
    "\n",
    "To get the theoretical insight about the DSM solution, we consider the score network is an one-layer linear network, e.g., \n",
    "\n",
    "$s_\\mathbf{w}(\\tilde{x}) = \\mathbf{w}^\\top \\tilde{x}$ \n",
    "\n",
    "with $\\mathbf{w}$ the feedforward weights to the score network.\n",
    "\n",
    "Taking the gradient over $\\mathbf{w}$ and set it to zero\n",
    "\n",
    "$\\frac{\\partial J}{\\partial \\mathbf{w}} = \\mathbb{E}_{q(\\tilde{x},x)} \\{ \n",
    "    [\\mathbf{w}^\\top \\tilde{x} - \\nabla_{\\tilde{x}} \\ln q_\\sigma(\\tilde{x}|x)] \\cdot \\tilde{x}^\\top \\}= 0$\n",
    "\n",
    "which is equivalent to \n",
    "\n",
    "$\\mathbf{w}^\\top \\mathbb{E}(\\tilde{x} \\tilde{x}^\\top) = \\mathbb{E} [\\nabla_{\\tilde{x}} \\ln q_\\sigma(\\tilde{x}|x) \\cdot \\tilde{x}^\\top]$\n",
    "\n",
    "Finally\n",
    "\n",
    "$\\mathbf{w}^\\top = [\\mathbb{E}(\\tilde{x} \\tilde{x}^\\top)]^{-1} \\mathbb{E} [\\nabla_{\\tilde{x}} \\ln q_\\sigma(\\tilde{x}|x) \\cdot \\tilde{x}^\\top]$\n",
    "\n",
    "This solution form is quite common and can be learned via biologically plausible learning rule, e.g., studies from Dmitri Chklovski and Cengiz Pehlevan.\n",
    "\n",
    "\n",
    "#### Insight\n",
    "We can regard $\\tilde{x}$ as congruent neurons' responses, and $\\nabla_{\\tilde{x}} \\ln q_\\sigma(\\tilde{x}|x)$ the approximated score by the inputs from congruent neurons to opposite neurons.\n",
    "Therefore the udpate of weights $\\mathbf{w}$ only uses local information.\n",
    "\n",
    "#### Theoretical analysis of $w$ in the score network\n",
    "\n",
    "Suppose the true score function is \n",
    "\n",
    "$$ \\nabla_{\\tilde{x}} \\ln q_\\sigma(\\tilde{x}|x) = \\sigma^{-2} (x - \\tilde{x})$$\n",
    "\n",
    "Then \n",
    "\n",
    "$$ \\mathbb{E} [\\nabla_{\\tilde{x}} \\ln q_\\sigma(\\tilde{x}|x) \\cdot \\tilde{x}^\\top] \n",
    " = \\mathbb{E} [ \\sigma^{-2} (x - \\tilde{x})\\cdot \\tilde{x}^\\top], \\\\\n",
    " = \\sigma^{-2} [ x \\mathbb{E}(\\tilde{x}) - \\mathbb{E}(\\tilde{x}\\tilde{x}^\\top) ]\n",
    "$$\n",
    "where \n",
    "$\\mathbb{E}(\\tilde{x}) = x$, and \n",
    "$\\mathbb{E}(\\tilde{x}\\tilde{x}^\\top) = x^2 + \\sigma^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A toy example: linear Score Function with Gaussian data distribution\n",
    "\n",
    "Problem setting:\n",
    "- Data follows a normal distribution: $x \\sim \\mathcal{N}(\\mu, \\sigma^2)$\n",
    "- We perturb it with noise: $\\tilde{x} = x + \\epsilon$ where $\\epsilon \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2)$\n",
    "- We want to learn a linear score function: $s_\\theta(\\tilde{x}) = w\\tilde{x} + b$\n",
    "\n",
    "Our goal is to find the optimal values of $w$ and $b$.\n",
    "\n",
    "####  __1. The true Score Function__\n",
    "\n",
    "For a normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$, the probability density function is:\n",
    "\n",
    "$$p(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$$\n",
    "\n",
    "The true score function:\n",
    "\n",
    "$$\\nabla_x \\log p(x) = \\frac{\\mu-x}{\\sigma^2}$$\n",
    "\n",
    "\n",
    "####  __2. Denoising score matching and linear score function__\n",
    "\n",
    "In denoising score matching, we train the score network to predict the noise direction. The objective function is:\n",
    "\n",
    "$$\\mathcal{L}(\\theta) \n",
    "= \\mathbb{E}_{x \\sim p(x), \\epsilon \\sim \\mathcal{N}(0,\\sigma_\\epsilon^2)} \n",
    "\\left[\\left\\|s_\\theta(x+\\epsilon) - \\frac{\\epsilon}{\\sigma_\\epsilon^2}\\right\\|^2\\right]$$\n",
    "\n",
    "For our linear model $s_\\theta(\\tilde{x}) = w\\tilde{x} + b$, we can find the optimal parameters by taking derivatives of the loss with respect to $w$ and $b$ and setting them to zero.\n",
    "\n",
    "First, let's rewrite our objective function for the scalar case:\n",
    "\n",
    "$$\\mathcal{L}(w,b) = \\mathbb{E}_{x,\\epsilon}\\left[\\left(w(x+\\epsilon) + b - \\frac{\\epsilon}{\\sigma_\\epsilon^2}\\right)^2\\right]$$\n",
    "\n",
    "Taking derivatives:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial w} \n",
    "= \\mathbb{E}_{x,\\epsilon}\\left[2(w(x+\\epsilon) + b - \\frac{\\epsilon}{\\sigma_\\epsilon^2})(x+\\epsilon)\\right]\n",
    "\\propto \\mathbb{E}_{x,\\epsilon}\\left[(w(x+\\epsilon) + b - \\frac{\\epsilon}{\\sigma_\\epsilon^2})(x+\\epsilon)\\right] = 0\n",
    "$$\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial b} \n",
    "= \\mathbb{E}_{x,\\epsilon}\\left[2(w(x+\\epsilon) + b - \\frac{\\epsilon}{\\sigma_\\epsilon^2})\\right]\n",
    "\\propto \\mathbb{E}_{x,\\epsilon}\\left[w(x+\\epsilon) + b - \\frac{\\epsilon}{\\sigma_\\epsilon^2}\\right] = 0\n",
    "$$\n",
    "\n",
    "- __Solving b__\n",
    "\n",
    "    From the second equation:\n",
    "    $$w\\mathbb{E}[x+\\epsilon] + b - \\mathbb{E}\\left[\\frac{\\epsilon}{\\sigma_\\epsilon^2}\\right] = 0$$\n",
    "\n",
    "    Since $\\mathbb{E}[\\epsilon] = 0$, we have:\n",
    "    $$w\\mu + b = 0$$\n",
    "    $$b = -w\\mu$$\n",
    "\n",
    "- __Solving w__\n",
    "\n",
    "    From the first equation, expanding:\n",
    "    $$\\mathbb{E}[w(x+\\epsilon)^2 + b(x+\\epsilon) - \\frac{\\epsilon(x+\\epsilon)}{\\sigma_\\epsilon^2}] = 0$$\n",
    "\n",
    "    Substituting $b = -w\\mu$:\n",
    "    $$w\\mathbb{E}[(x+\\epsilon)^2] - w\\mu\\mathbb{E}[x+\\epsilon] - \\mathbb{E}\\left[\\frac{\\epsilon x + \\epsilon^2}{\\sigma_\\epsilon^2}\\right] = 0$$\n",
    "\n",
    "    Simplifying:\n",
    "    $$w(\\mathbb{E}[x^2] + \\mathbb{E}[\\epsilon^2]) - w\\mu^2 - \\frac{\\mathbb{E}[\\epsilon^2]}{\\sigma_\\epsilon^2} = 0$$\n",
    "\n",
    "    Given $\\mathbb{E}[x^2] = \\sigma^2 + \\mu^2$ and $\\mathbb{E}[\\epsilon^2] = \\sigma_\\epsilon^2$:\n",
    "    $$w(\\sigma^2 + \\mu^2 + \\sigma_\\epsilon^2) - w\\mu^2 - 1 = 0$$ \n",
    "    $$w = \\frac{1}{\\sigma^2 + \\sigma_\\epsilon^2}$$\n",
    "\n",
    "#### __3. Score function in limit of with small noise perturbation__\n",
    "\n",
    "As $\\sigma_\\epsilon^2 \\to 0$ (small noise limit), we get:\n",
    "$$w = \\frac{1}{\\sigma^2} \\cdot \\frac{\\sigma^2}{\\sigma^2 + \\sigma_\\epsilon^2} \\approx \\frac{1}{\\sigma^2}$$\n",
    "\n",
    "Since $b = -w\\mu$:\n",
    "$$b = -\\frac{\\mu}{\\sigma^2}$$\n",
    "\n",
    "However, there's a sign discrepancy with our expected score function. This is because in the denoising formulation, the score approximates the negative gradient direction. Therefore the correct expressions are:\n",
    "\n",
    "$$w = -\\frac{1}{\\sigma^2}$$\n",
    "$$b = \\frac{\\mu}{\\sigma^2}$$\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The optimal parameters for a linear score function $s_\\theta(\\tilde{x}) = w\\tilde{x} + b$ to approximate the score of a normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$ are:\n",
    "\n",
    "$$w = -\\frac{1}{\\sigma^2}$$\n",
    "$$b = \\frac{\\mu}{\\sigma^2}$$\n",
    "\n",
    "This aligns with our intuition: the score function should point toward the mean with strength inversely proportional to the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A demo code of the diffusion model\n",
    "# Forward process: Langevin sampling dynamics of a given (posterior) distribution p(x)\n",
    "#                  Diffuse from uniform distribution to p(x)\n",
    "# Backward process: the score function is approximated via sampling\n",
    "\n",
    "# Wen-Hao Zhang, wenhao.zhang@utsouthwestern.edu\n",
    "# Feb 20, 2024\n",
    "# UT Southwestern, Dallas, TX\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Parameter of the distribution to be sampled\n",
    "mu = 0;\n",
    "sigma = 1;\n",
    "tau = 1;\n",
    "num_trials = int(5e3);\n",
    "\n",
    "# Simulation parameters\n",
    "dt = 0.01 * tau;\n",
    "tLen = 10;\n",
    "\n",
    "# Assemble parameters into a dictionary\n",
    "ParamDiffusion = {'mu': mu, 'sigma': sigma, 'tau': tau, 'dt': dt};\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Define the forward process transition kernel\n",
    "def logProbForward(x1, x2, Params):\n",
    "    # Unnormalized forward transition probability\n",
    "    # x1: current state\n",
    "    # x2: next state\n",
    "    diffusion_term = (Params['mu'] - x1) / Params['tau'] / Params['sigma']**2\n",
    "    trans_mean = x1 + diffusion_term * Params['dt'];\n",
    "    trans_var = 2 * Params['dt'] / Params['tau'];\n",
    "    \n",
    "    logProb = - (x2 - trans_mean)**2 / (2 * trans_var);\n",
    "    return logProb;\n",
    "\n",
    "def logProbBackward(x1, x2, Params):\n",
    "    # Unnormalized backward transition probability\n",
    "    # x1: current state\n",
    "    # x2: next state\n",
    "    \n",
    "    diffusion_term = (Params['mu'] - x1) / Params['tau'] / Params['sigma']**2\n",
    "    trans_mean_forward = x1 + diffusion_term * Params['dt'];\n",
    "    trans_var = 2 * Params['dt'] / Params['tau'];\n",
    "    \n",
    "    # Approximate gradient of the marginal distribution at time t via sampling\n",
    "    # p(x_t+1) = \\int q(x_t+1 | x_t) p(x_t) dx_t \n",
    "    #          \\approx q(x_t+1 | \\tilde{x}_t), where \\tilde{x}_t ~ p(x_t)\n",
    "    # Therefore \\nabla_x_t log p(x_t+1) \\approx \\nabla_x_t q(x_t+1 | \\tilde{x}_t)\n",
    "    \n",
    "    score_approx =  (trans_mean_forward - x2) / trans_var;\n",
    "    trans_mean_backward = trans_mean_forward - trans_var * score_approx * Params['dt'];\n",
    "    \n",
    "    logProb = - (x2 - trans_mean_backward)**2 / (2 * trans_var); \n",
    "    return logProb;\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Simulate the forward and backward processes\n",
    "\n",
    "# Initialization\n",
    "num_steps = int(tLen / dt);\n",
    "# x = np.zeros(num_steps + 1);\n",
    "# ELBO = np.zeros(num_steps + 1);\n",
    "x = np.zeros((num_steps + 1, num_trials));\n",
    "ELBO = np.zeros((num_steps + 1, num_trials));\n",
    "logProb_Back_array = np.zeros((num_steps + 1, num_trials));\n",
    "logProb_Forward_array = np.zeros((num_steps + 1, num_trials));\n",
    "x[0,:] = 0;\n",
    "# x[0,:] = np.random.normal(mu, sigma, (1,num_trials));\n",
    "\n",
    "# Simulate a Langevin dynamics\n",
    "for iter in range(0, num_steps):\n",
    "    # dx = sigma**-2 * (mu - x[iter]) / tau * dt + np.sqrt(2/tau * dt) * np.random.normal(0, 1, 1);\n",
    "    dx = sigma**-2 * (mu - x[iter,:]) / tau * dt + np.sqrt(2/tau * dt) * np.random.normal(0, 1, (1,num_trials));\n",
    "    x[iter+1,:] = x[iter,:] + dx;\n",
    "    \n",
    "    ELBO[iter+1,:] = ELBO[iter,:] + logProbBackward(x[iter,:], x[iter+1,:], ParamDiffusion) - logProbForward(x[iter,:], x[iter+1,:], ParamDiffusion);\n",
    "    logProb_Back_array[iter+1,:] = logProbBackward(x[iter,:], x[iter+1,:], ParamDiffusion);\n",
    "    logProb_Forward_array[iter+1,:] = logProbForward(x[iter], x[iter+1,:], ParamDiffusion);\n",
    "    \n",
    "# Plot the position over time\n",
    "trial_example = np.random.randint(0, num_trials);\n",
    "plt.plot(np.arange(num_steps+1) * dt, x[:,trial_example])\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Position')\n",
    "plt.title('Langevin Dynamics Simulation')\n",
    "plt.show()\n",
    "\n",
    "ELBO_mean = np.log(np.mean(np.exp(ELBO), axis=1));\n",
    "# plt.plot(np.arange(num_steps+1) * dt, np.mean(ELBO, axis=1))\n",
    "plt.plot(np.arange(num_steps+1) * dt, ELBO_mean)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('ELBO')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(np.arange(num_steps+1) * dt, logProb_Forward_array[:,trial_example])\n",
    "# plt.plot(np.arange(num_steps+1) * dt, logProb_Back_array[:,trial_example])\n",
    "# plt.plot(np.arange(num_steps+1) * dt, logProb_Back_array[:,trial_example] - logProb_Forward_array[:,trial_example])\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('log probability')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Demo of the Denoising Score Matching (DSM)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# --------------------------------------------------\n",
    "# Parameter of the data distibution\n",
    "mu0 = 0;\n",
    "sigma0 = 2;\n",
    "\n",
    "# Parameter of the transition probability\n",
    "mu = 0;\n",
    "sigma = 1;\n",
    "tau = 1;\n",
    "num_data = int(50);\n",
    "num_samples = int(1e4);\n",
    "\n",
    "# Simulation parameters\n",
    "dt = 0.01 * tau;\n",
    "tLen = 10;\n",
    "\n",
    "# Assemble parameters into a dictionary\n",
    "ParamDiffusion = {'mu': mu, 'sigma': sigma, 'tau': tau, 'dt': dt};\n",
    "\n",
    "x = np.random.normal(mu0, sigma0, (num_data,1))\n",
    "# --------------------------------------------------\n",
    "# Approximated the score of the data distribution via sampling\n",
    "# p(x_t+1) = \\int q(x_t+1 | x_t) p(x_t) dx_t \n",
    "#          \\approx q(x_t+1 | \\tilde{x}_t), where \\tilde{x}_t ~ p(x_t)\n",
    "# Therefore \\nabla_x_t log p(x_t+1) \\approx \\nabla_x_t q(x_t+1 | \\tilde{x}_t)\n",
    "\n",
    "diffusion_term = (mu - x) / tau / sigma**2\n",
    "trans_var = 2 * dt / tau;\n",
    "\n",
    "# Noise perturbation via one step of Langevin dynamics\n",
    "x_perturb = x + diffusion_term * dt + np.sqrt(trans_var) * np.random.normal(0, 1, (num_data, num_samples));\n",
    "score_approx =  (x + diffusion_term * dt - x_perturb) / trans_var;\n",
    "\n",
    "# x_perturb = x + np.random.normal(0, 1, (num_data, num_samples));\n",
    "score_approx =  (x - x_perturb);\n",
    "\n",
    "score_approx_mean = np.mean(score_approx, axis = 1);\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# A linear model to approximate the score function\n",
    "\n",
    "#  Augment x_perturb by inserting an offset input\n",
    "# x_perturb_aug = np.expand_dims(x_perturb, axis=1)\n",
    "# x_perturb_aug = np.concatenate((x_perturb_aug, np.ones(x_perturb_aug.shape)), axis = 1);\n",
    "\n",
    "w_score = np.linalg.inv(np.dot(x_perturb, x_perturb.transpose())) * np.dot(score_approx, x_perturb.transpose());\n",
    "\n",
    "# --------------------------------------------------\n",
    "# The true score of the data distribution\n",
    "score_true = (mu0 - x) / sigma0**2;\n",
    "plt.plot(x, score_true)\n",
    "plt.plot(x, score_approx, '.')\n",
    "# plt.plot(x, score_approx_mean)\n",
    "plt.twinx().plot(x, w_score * x, 'o')\n",
    "plt.xlabel('Data x')\n",
    "plt.ylabel('Score (gradient of the data distribution)')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x, w_score, 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_perturb_aug = np.concatenate((x_perturb, np.ones((num_data, num_samples))), axis = 0);\n",
    "x_perturb_aug = np.expand_dims(x_perturb, axis=1)\n",
    "\n",
    "# print(np.ones(tt.shape).shape)\n",
    "x_perturb_aug = np.concatenate((x_perturb_aug, np.ones(x_perturb_aug.shape)), axis = 1);\n",
    "print(x_perturb_aug.shape)\n",
    "# print(tt.shape)\n",
    "# print(tt_aug.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
